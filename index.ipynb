{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll calculate various evaluation metrics to compare to evaluate classifier performance!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Read and interpret results using a Confusion Matrix\n",
    "* Calculate and interpret precision and recall and evaluation metrics for classification\n",
    "* Calculate and interpret accuracy and f1-score as evaluation metrics for classification\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "For this lab, you're going to read in a DataFrame containing various predictions from different models, as well as the ground-truth labels for the dataset that each model was making predictions on. You'll also write various functions to help you easily calculate important evaluation metrics such as **_Precision_**, **_Recall_**, **_Accuracy_**, and **_F1-Score_**.\n",
    "\n",
    "Let's start by reading in our dataset. You'll find the dataset stored in `'model_performance.csv'`. In the cell below, use pandas to read this dataset into a DataFrame, and inspect the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\rrubel\\Documents\\Materials\\section27\\dsc-3-27-08-evaluation-metrics-lab-online-ds-sp-000\\model_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of model predictions from 3 different models, as well as the corresponding labels for row in the dataset. \n",
    "\n",
    "In the cell below, store each of the following predictions and labels in separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_preds = df['Model 1 Predictions']\n",
    "model2_preds = df['Model 2 Predictions']\n",
    "model3_preds = df['Model 3 Predictions']\n",
    "labels = df['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now, let's get started by building a confusion matrix!\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "In the cell below, complete the `conf_matrix` function.  This function should:\n",
    "\n",
    "* Take in 2 arguments: \n",
    "    * `y_true`, an array of labels\n",
    "    * `y_pred`, an array of model predictions\n",
    "* Return a Confusion Matrix in the form of a dictionary, where the keys are `'TP', 'TN', 'FP', 'FN'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y_true, y_pred):\n",
    "    d = dict.fromkeys(['TP', 'TN', 'FP', 'FN'], 0)\n",
    "    for i in list(range(0,len(y_true))):\n",
    "        if y_true[i] == 1 and y_pred[i] == 1:\n",
    "            d['TP'] += 1\n",
    "        elif y_true[i] == 1 and y_pred[i] == 0:\n",
    "            d['FP'] += 1\n",
    "        elif y_true[i] == 0 and y_pred[i] == 0:\n",
    "            d['TN'] += 1\n",
    "        elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "            d['FN'] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's double check that our function was created correctly by creating confusion matrices for each of our 3 models. Expected outputs have been provided for you to check your results against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 6168, 'TN': 2654, 'FP': 346, 'FN': 832}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Expected Output: {'TP': 6168, 'TN': 2654, 'FP': 346, 'FN': 832}\n",
    "model1_confusion_matrix = conf_matrix(model1_preds, labels)\n",
    "model1_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 3914, 'TN': 1659, 'FP': 1341, 'FN': 3086}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Expected Output: {'TP': 3914, 'TN': 1659, 'FP': 1341, 'FN': 3086}\n",
    "model2_confusion_matrix = conf_matrix(model2_preds, labels)\n",
    "model2_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 5505, 'TN': 2319, 'FP': 681, 'FN': 1495}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Expected Output: {'TP': 5505, 'TN': 2319, 'FP': 681, 'FN': 1495}\n",
    "model3_confusion_matrix = conf_matrix(model3_preds, labels)\n",
    "model3_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Our Work with sklearn\n",
    "\n",
    "To check our work, let's make use the the `confusion_matrix()` function found in `sklearn.metrics` to create some confusion matrices and make sure that sklearn's results match up with our own.\n",
    "\n",
    "In the cells below, import the `confusion_matrix()` function, use it to create a confusion matrix for each of our models, and then compare the results with the confusion matrices we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2654  832]\n",
      " [ 346 6168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model1_sk_cm = confusion_matrix(model1_preds, labels)\n",
    "print(model1_sk_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1659 3086]\n",
      " [1341 3914]]\n"
     ]
    }
   ],
   "source": [
    "model2_sk_cm = confusion_matrix(model2_preds, labels)\n",
    "print(model2_sk_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2319 1495]\n",
      " [ 681 5505]]\n"
     ]
    }
   ],
   "source": [
    "model3_sk_cm = confusion_matrix(model3_preds, labels)\n",
    "print(model3_sk_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Visualizing Confusion Matrices\n",
    "\n",
    "In the cells below, use the visualization function shown in the **_Confusion Matrices_** lesson to visualize each of the confusion matrices created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<matplotlib.axis.YTick at 0x16422fedeb8>,\n",
       "  <matplotlib.axis.YTick at 0x164253a5588>],\n",
       " Text(0, 0.5, 'True label'),\n",
       " [<matplotlib.axis.XTick at 0x16422fedf98>,\n",
       "  <matplotlib.axis.XTick at 0x1642337b860>],\n",
       " Text(0.5, 0, 'Predicted label')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(model1_sk_cm, interpolation='nearest')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(model1_sk_cm.shape[1]),\n",
    "           yticks=np.arange(model1_sk_cm.shape[0]),\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<matplotlib.axis.YTick at 0x1642655d7b8>,\n",
       "  <matplotlib.axis.YTick at 0x1642655d0f0>],\n",
       " Text(0, 0.5, 'True label'),\n",
       " [<matplotlib.axis.XTick at 0x16426557a58>,\n",
       "  <matplotlib.axis.XTick at 0x16426557390>],\n",
       " Text(0.5, 0, 'Predicted label')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(model2_sk_cm, interpolation='nearest')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(model2_sk_cm.shape[1]),\n",
    "           yticks=np.arange(model2_sk_cm.shape[0]),\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<matplotlib.axis.YTick at 0x164265eb198>,\n",
       "  <matplotlib.axis.YTick at 0x164265e5a20>],\n",
       " Text(0, 0.5, 'True label'),\n",
       " [<matplotlib.axis.XTick at 0x164265e5358>,\n",
       "  <matplotlib.axis.XTick at 0x164265d9ba8>],\n",
       " Text(0.5, 0, 'Predicted label')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEKCAYAAACL0zmLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFaZJREFUeJzt3XuwXVVhx/HvLwkvAQVEKAYsqUQqdQpijEztA8UGRGtoKzNQq1EZUy22WltbtHZoVaZ2OlM61GeUlGBbMa1SGKWmEd8O7zcRMQGLpDBGDKIlCuTeX//Y6+ox3HPu3uaee88++/eZ2ZOz19lnr3Vh5jdr7bX32rJNRESXLJjvBkREzLUEX0R0ToIvIjonwRcRnZPgi4jOSfBFROck+CKicxJ8EdE5Cb6I6JxF892AXov22dd77n/QfDcjGli0Y3K+mxAN/OiR7/HoYw9rd85x8gv29Xe3T9Q69oZbH9lg+5TdqW8YRir49tz/II7+3T+Z72ZEA0++dcd8NyEauPaWD+z2Ob67fYJrNzyt1rELD9t88G5XOAQjFXwRMfoMTNLunn6CLyIaMeYx1xvqjqoEX0Q0lh5fRHSKMRMtX84uwRcRjU2S4IuIDjEwkeCLiK5Jjy8iOsXAY7nGFxFdYpyhbkR0jGGi3bmX4IuIZqonN9otwRcRDYkJdmudg3mX4IuIRqrJjQRfRHRIdR9fgi8iOmYyPb6I6JL0+CKic4yYaPlbK9rd+oiYF5NWrW0mkv5H0m2SbpZ0fSk7SNJGSZvLvweWckm6QNIWSbdKOr7nPKvK8ZslrZqp3gRfRDRixKNeWGur6QW2j7O9rOyfA1xpeylwZdkHeDGwtGyrgQ9AFZTAucDzgOXAuVNh2U+CLyIaqW5gXlBr+xmtBNaVz+uA03rKL3blauAASYcBJwMbbW+3/SCwERj4gqMEX0Q0NlFuYp5pq8HAf0u6QdLqUnao7fsByr+HlPLFwL09v91ayvqV95XJjYhoxBYTrt1nOnjq2l2xxvaanv3n275P0iHARklfH3Cu6ZLUA8r7SvBFRGOT9W9neaDn2t3j2L6v/LtN0qVU1+i+Lekw2/eXoey2cvhW4Iienx8O3FfKT9yl/AuDGpWhbkQ0Uk1uLKq1DSJpX0n7T30GVgC3A5cDUzOzq4DLyufLgVeV2d0TgIfKUHgDsELSgWVSY0Up6ys9vohoZGpyYxYcClwqCaos+jfbn5F0HbBe0lnAt4DTy/FXAKcCW4AdwGsAbG+X9C7gunLcO21vH1Rxgi8iGpuYhUfWbN8NHDtN+XeBk6YpN3B2n3OtBdbWrTvBFxGNjMOTGwm+iGhssv6s7khK8EVEI9UiBQm+iOgQIx6r/zjaSErwRUQjNk1uYB5JCb6IaEhNbmAeSQm+iGjEpMcXER2UyY2I6BRTb5HRUZbgi4hGqtdLtjs62t36iJgHeaF4RHSMyZMbEdFB6fFFRKfYSo8vIrqlmtzII2sR0SmN3rkxkhJ8EdFINbmRa3wR0TF5ciMiOiVPbkREJ83Sy4bmTYIvIhqx4bHJBF9EdEg11E3wRUTHtP3JjaHGtqRTJN0paYukc4ZZV0TMjanbWepso2poPT5JC4H3Ab8JbAWuk3S57a8Nq86ImAvtH+oOs/XLgS2277b9KHAJsHKI9UXEHJks792YaRtVw7zGtxi4t2d/K/C8IdYXEXOgmtXNs7r9TBf3ftxB0mpgNcAe+x04xOZExGwYhxuYhznU3Qoc0bN/OHDfrgfZXmN7me1li/bZd4jNiYjZkqFuf9cBSyUtAf4XOAP4vSHWFxFzIIsUDGB7p6Q3AhuAhcBa25uGVV9EzJ22z+oO9QZm21cAVwyzjoiYW7bYmeCLiK7JUDciOiXX+CKikxJ8EdEp43AfX4IvIhob5Xv06mj31ExEzDkbdk4uqLXVIWmhpJskfarsL5F0jaTNkj4uac9SvlfZ31K+P7LnHG8r5XdKOnmmOhN8EdHYLC9L9Sbgjp79vwPOt70UeBA4q5SfBTxo+yjg/HIcko6hekDil4BTgPeX1aH6SvBFRCNT1/hmI/gkHQ68BPhI2RfwQuA/yiHrgNPK55Vln/L9SeX4lcAlth+x/U1gC9XqUH0l+CKiMVu1NuBgSdf3bKt3OdU/An8OTJb9JwPfs72z7G+lWukJelZ8Kt8/VI6fbiWoxQyQyY2IaKzB5MYDtpdN94WklwLbbN8g6cSp4mkO9Qzf1VoJqleCLyIasWftPr7nAy+TdCqwN/BEqh7gAZIWlV5d76pOUys+bZW0CHgSsJ2aK0H1ylA3IhoSE5MLam2D2H6b7cNtH0k1OfE5268APg+8vBy2CrisfL687FO+/5xtl/IzyqzvEmApcO2gutPji4jGPNwbmP8CuETSu4GbgAtL+YXARyVtoerpnVG1xZskrQe+BuwEzrY9MaiCBF9ENDKMZ3VtfwH4Qvl8N9PMytr+EXB6n9+fB5xXt74EX0Q04+o6X5sl+CKisbY/spbgi4hGXCY32izBFxGNZagbEZ0z5FndoUvwRUQjdoIvIjooC5FGROfkGl9EdIoRk5nVjYiuaXmHL8EXEQ2N8+SGpCcO+qHt789+cyKiFVre5RvU49vE4xf5m9o38LQhtisiRtjY9vhsH9Hvu4joLgOTk+0OvlpTM5LOkPT28vlwSc8ZbrMiYmQZsOptI2rG4JP0XuAFwCtL0Q7gg8NsVESMNrveNqrqzOr+iu3jJd0EYHv71At+I6KjRjjU6qgTfI9JWkD5UyU9mZ+8Ci4iOketn9yoc43vfcAngKdI+hvgK5Q3mEdER7nmNqJm7PHZvljSDcCLStHptm8fbrMiYmQZ3PJZ3bpPbiwEHqPK8HY/pBcRs6DdwVdnVvcvgY8BT6V6Ue+/SXrbsBsWESNs3Ie6wO8Dz7G9A0DSecANwN8Os2ERMcJGONTqqBN89+xy3CLg7uE0JyJG3tQNzC02aJGC86n+xB3AJkkbyv4KqpndiOioUb45uY5BPb6pmdtNwKd7yq8eXnMiohXGdVbX9oVz2ZCIaA+NcY8PAElPB84DjgH2niq3/YwhtisiRtWIz9jWUeeevIuAf6a6cefFwHrgkiG2KSJGWs2VWUZ4AqRO8D3B9gYA23fZfgfVai0R0VUduI/vEUkC7pL0euB/gUOG26yIGGktX6akTvD9CbAf8MdU1/qeBLx2mI2KiBE2zvfxTbF9Tfn4A36yGGlEdNjYzupKupQBo3TbvzOUFkXE6BvX4APeO2etiIiYQ4NuYL5yLhsCsOg7D3Pwh66a62pjN2y47+b5bkI0sPzk787Kedo+1M3aehHRjKkeWauzDSBpb0nXSrpF0qaywjuSlki6RtJmSR+fesePpL3K/pby/ZE953pbKb9T0skz/QkJvohobnbu43sEeKHtY4HjgFMknUD1aovzbS8FHgTOKsefBTxo+yjg/HIcko4BzgB+CTgFeL+khYMqrh18kvaqe2xEjDe53jaIK/9Xdvcom4EXAv9RytcBp5XPK8s+5fuTyj3GK4FLbD9i+5vAFmD5oLrrrMC8XNJtwOayf6ykf5rpdxExxur3+A6WdH3Ptrr3NJIWSroZ2AZsBO4Cvmd7ZzlkK7C4fF4M3AtQvn8IeHJv+TS/mVadG5gvAF4K/Gep8BZJeWQtosvqT248YHtZ39PYE8Bxkg4ALgWeOaC26S4aekB5X3WGugts37NL2USN30XEGKo7zG0y82v7e8AXgBOAAyRNdcoOB+4rn7cCRwCU758EbO8tn+Y306oTfPdKWg64dEvfDHyj1l8TEeNpdmZ1n1J6ekjah+oVtncAnwdeXg5bBVxWPl9e9inff862S/kZZdZ3CbAUuHZQ3XWGum+gGu4+Dfg28NlSFhEdNUv38R0GrCszsAuA9bY/JelrwCWS3g3cBEwtinwh8FFJW6h6emcA2N4kaT3wNWAncHYZQvdV51ndbVMVREQAs/LImu1bgWdPU34308zK2v4RcHqfc51HtYhKLXVWYP4w0/yZtldPc3hEjLuG1+9GUZ2h7md7Pu8N/DY/PXUcEV0z7sFn++O9+5I+SnW/TUR0lFq+EOnP8sjaEuDnZ7shERFzpc41vgf5Scd2AdVsyjnDbFREjLhxHuqW5+COpXrPBsBkuW8mIrpqDCY3Bg51S8hdanuibC3/cyNiVrT8LWt1rvFdK+n4obckItqj5cE36J0bi8oKCL8KvE7SXcDDVA8E23bCMKKDRPtndQdd47sWOJ6frIUVETEW1/gGBZ8AbN81R22JiLYY4+B7iqS39PvS9j8MoT0R0QZjHHwLgf2YfpG/iOiwcR7q3m/7nXPWkohojzEOvvT0IuLxPN6zuifNWSsiol3Gtcdne/tcNiQi2mOcr/FFREwvwRcRnTLij6PVkeCLiEZEhroR0UEJvojongRfRHROgi8iOmXMV2eJiJhegi8iumacH1mLiJhWhroR0S25gTkiOinBFxFdkic3IqKTNNnu5EvwRUQzucYXEV2UoW5EdE+CLyK6Jj2+iOielgffgvluQES0THnLWp1tEElHSPq8pDskbZL0plJ+kKSNkjaXfw8s5ZJ0gaQtkm6VdHzPuVaV4zdLWjXTn5Dgi4hGpu7jq7PNYCfwp7afCZwAnC3pGOAc4ErbS4Eryz7Ai4GlZVsNfACqoATOBZ4HLAfOnQrLfhJ8EdGcXW8beArfb/vG8vkHwB3AYmAlsK4ctg44rXxeCVzsytXAAZIOA04GNtrebvtBYCNwyqC6c40vIhprMLlxsKTre/bX2F7zuPNJRwLPBq4BDrV9P1ThKOmQcthi4N6en20tZf3K+0rwRUQzzW5gfsD2skEHSNoP+ATwZtvfl9T30D6t6Vfe19CGupLWStom6fZh1RER82M2JjcAJO1BFXr/avuTpfjbZQhL+XdbKd8KHNHz88OB+waU9zXMa3wXMcM4OyLaaZZmdQVcCNxh+x96vrocmJqZXQVc1lP+qjK7ewLwUBkSbwBWSDqwTGqsKGV9DW2oa/tLZdweEePEzDhxUdPzgVcCt0m6uZS9HXgPsF7SWcC3gNPLd1cApwJbgB3AawBsb5f0LuC6ctw7bW8fVPG8X+OTtJpqapq9ecI8tyYi6piNJzdsf4Xpr88BnDTN8QbO7nOutcDaunXP++0sttfYXmZ72R7sNd/NiYg6XHMbUfPe44uIdslCpBHRPXbrFyId5u0sHwOuAo6WtLVcqIyIcZCh7vRsnzmsc0fE/MpQNyK6xUDLh7oJvohort25l+CLiOYy1I2Izmn7rG6CLyKaGfEZ2zoSfBHRSHUDc7uTL8EXEc3VWHJqlCX4IqKx9PgioltyjS8iuqf9z+om+CKiuQx1I6JTXO99GqMswRcRzaXHFxGd0+7cS/BFRHOabPdYN8EXEc2Y3MAcEd0inBuYI6KDEnwR0TkJvojolFzji4guyqxuRHSMM9SNiI4xCb6I6KB2j3QTfBHRXO7ji4juSfBFRKfYMNHusW6CLyKaS48vIjonwRcRnWIg79yIiG4xuN3X+BbMdwMiomVMNblRZ5uBpLWStkm6vafsIEkbJW0u/x5YyiXpAklbJN0q6fie36wqx2+WtGqmehN8EdGcXW+b2UXAKbuUnQNcaXspcGXZB3gxsLRsq4EPQBWUwLnA84DlwLlTYdlPgi8impul4LP9JWD7LsUrgXXl8zrgtJ7yi125GjhA0mHAycBG29ttPwhs5PFh+lNyjS8iGmq0SMHBkq7v2V9je80MvznU9v0Atu+XdEgpXwzc23Pc1lLWr7yvBF9ENGOg/rJUD9heNks1q09r+pX3laFuRDQ3e9f4pvPtMoSl/LutlG8Fjug57nDgvgHlfSX4IqIhz9qsbh+XA1Mzs6uAy3rKX1Vmd08AHipD4g3ACkkHlkmNFaWsrwx1I6IZg2fpPj5JHwNOpLoWuJVqdvY9wHpJZwHfAk4vh18BnApsAXYArwGwvV3Su4DrynHvtL3rhMlPSfBFRHOz9OSG7TP7fHXSNMcaOLvPedYCa+vWm+CLiObyrG5EdIrdZFZ3JCX4IqK59PgioluMJybmuxG7JcEXEc1kWaqI6KSWL0uV4IuIRgw4Pb6I6BS3fyHSBF9ENNb2yQ15hKalJX0HuGe+2zEEBwMPzHcjopFx/X/287afsjsnkPQZqv8+dTxge+DaePNhpIJvXEm6fhaX5ok5kP9n4y2rs0RE5yT4IqJzEnxzY6altmP05P/ZGMs1vojonPT4IqJzEnxDJOkUSXeWFyCfM/MvYr5N94LrGD8JviGRtBB4H9VLkI8BzpR0zPy2Kmq4iBneyRrtl+AbnuXAFtt3234UuITqhcgxwvq84DrGTIJveBq/5Dgi5kaCb3gav+Q4IuZGgm94Gr/kOCLmRoJveK4DlkpaImlP4AyqFyJHxDxL8A2J7Z3AG6ne6H4HsN72pvltVcykvOD6KuBoSVvLS61jzOTJjYjonPT4IqJzEnwR0TkJvojonARfRHROgi8iOifB1yKSJiTdLOl2Sf8u6Qm7ca4TJX2qfH7ZoNVjJB0g6Q9/hjr+WtKf1S3f5ZiLJL28QV1HZkWVqCvB1y4/tH2c7WcBjwKv7/1Slcb/T21fbvs9Aw45AGgcfBGjKsHXXl8Gjio9nTskvR+4EThC0gpJV0m6sfQM94Mfrw/4dUlfAX5n6kSSXi3pveXzoZIulXRL2X4FeA/w9NLb/Pty3FslXSfpVkl/03OuvyxrEH4WOHqmP0LS68p5bpH0iV16sS+S9GVJ35D00nL8Qkl/31P3H+zuf8jongRfC0laRLXO322l6GjgYtvPBh4G3gG8yPbxwPXAWyTtDXwY+C3g14Cf63P6C4Av2j4WOB7YBJwD3FV6m2+VtAJYSrX01nHAcyT9uqTnUD2a92yqYH1ujT/nk7afW+q7A+h9UuJI4DeAlwAfLH/DWcBDtp9bzv86SUtq1BPxY4vmuwHRyD6Sbi6fvwxcCDwVuMf21aX8BKqFT78qCWBPqkewfhH4pu3NAJL+BVg9TR0vBF4FYHsCeEjSgbscs6JsN5X9/aiCcH/gUts7Sh11nk1+lqR3Uw2n96N6xG/KetuTwGZJd5e/YQXwyz3X/55U6v5GjboigARf2/zQ9nG9BSXcHu4tAjbaPnOX445j9pbFEvC3tj+0Sx1v/hnquAg4zfYtkl4NnNjz3a7ncqn7j2z3BiSSjmxYb3RYhrrj52rg+ZKOApD0BEnPAL4OLJH09HLcmX1+fyXwhvLbhZKeCPyAqjc3ZQPw2p5rh4slHQJ8CfhtSftI2p9qWD2T/YH7Je0BvGKX706XtKC0+ReAO0vdbyjHI+kZkvatUU/Ej6XHN2Zsf6f0nD4maa9S/A7b35C0Gvi0pAeArwDPmuYUbwLWlFVJJoA32L5K0lfL7SL/Va7zPRO4qvQ4/w/4fds3Svo4cDNwD9VwfCZ/BVxTjr+Nnw7YO4EvAocCr7f9I0kfobr2d6Oqyr8DnFbvv05EJauzRETnZKgbEZ2T4IuIzknwRUTnJPgionMSfBHROQm+iOicBF9EdE6CLyI65/8BahWySsgoi0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(model3_sk_cm, interpolation='nearest')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(model3_sk_cm.shape[1]),\n",
    "           yticks=np.arange(model3_sk_cm.shape[0]),\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Evaluation Metrics\n",
    "\n",
    "Now, we'll use our newly created confusion matrices to calculate some evaluation metrics. \n",
    "\n",
    "As a reminder, here are the equations for each evaluation metric we'll be calculating in this lab:\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$Precision = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$$\n",
    "\n",
    "### Recall\n",
    "\n",
    "$$Recall = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}}$$\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$Accuracy = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}}$$\n",
    "\n",
    "### F1-Score\n",
    "\n",
    "$$F1-Score = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall}$$\n",
    "\n",
    "In each of the cells below, complete the function to calculate the appropriate evaluation metrics. Use the output to fill in the following table: \n",
    "\n",
    "|  Model  | Precision | Recall | Accuracy | F1-Score |\n",
    "|:-------:|:---------:|:------:|:--------:|:--------:|\n",
    "| Model 1 |     0.94688363524716      |    0.8811428571428571    |     0.8822     |     0.9128311380790292     |\n",
    "| Model 2 |     0.744814462416746      |    0.5591428571428572    |     0.5573     |    0.6387596899224806      |\n",
    "| Model 3 |    0.8899127061105723      |   0.7864285714285715     |    0.7824      |     0.8349764902168968     |\n",
    "\n",
    "**_QUESTION:_** Which model performed the best? How do arrive at your answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94688363524716\n",
      "0.744814462416746\n",
      "0.8899127061105723\n"
     ]
    }
   ],
   "source": [
    "def precision(confusion_matrix):\n",
    "    p = (confusion_matrix['TP'])/((confusion_matrix['TP'])+(confusion_matrix['FP']))\n",
    "    return p\n",
    "print(precision(model1_confusion_matrix)) # Expected Output: 0.94688363524716\n",
    "print(precision(model2_confusion_matrix)) # Expected Output: 0.744814462416746\n",
    "print(precision(model3_confusion_matrix)) # Expected Output: 0.8899127061105723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811428571428571\n",
      "0.5591428571428572\n",
      "0.7864285714285715\n"
     ]
    }
   ],
   "source": [
    "def recall(confusion_matrix):\n",
    "    p = (confusion_matrix['TP'])/((confusion_matrix['TP'])+(confusion_matrix['FN']))\n",
    "    return p\n",
    "\n",
    "print(recall(model1_confusion_matrix)) # Expected Output: 0.8811428571428571\n",
    "print(recall(model2_confusion_matrix)) # Expected Output: 0.5591428571428572\n",
    "print(recall(model3_confusion_matrix)) # Expected Output: 0.7864285714285715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8822\n",
      "0.5573\n",
      "0.7824\n"
     ]
    }
   ],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    p = ((confusion_matrix['TP'])+(confusion_matrix['TN']))/(sum(confusion_matrix.values()))\n",
    "    return p\n",
    "\n",
    "print(accuracy(model1_confusion_matrix)) # Expected Output: 0.8822\n",
    "print(accuracy(model2_confusion_matrix)) # Expected Output: 0.5573\n",
    "print(accuracy(model3_confusion_matrix)) # Expected Output: 0.7824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9128311380790292\n",
      "0.6387596899224806\n",
      "0.8349764902168968\n"
     ]
    }
   ],
   "source": [
    "def f1(confusion_matrix):\n",
    "    p = 2*(precision(confusion_matrix)*recall(confusion_matrix))/(precision(confusion_matrix)+recall(confusion_matrix))\n",
    "    return p\n",
    "print(f1(model1_confusion_matrix)) # Expected Output: 0.9128311380790292\n",
    "print(f1(model2_confusion_matrix)) # Expected Output: 0.6387596899224806\n",
    "print(f1(model3_confusion_matrix)) # Expected Output: 0.8349764902168968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Job! Let's check our work with sklearn. \n",
    "\n",
    "## Calculating Metrics with sklearn\n",
    "\n",
    "Each of the metrics we calculated above are also available inside the `sklearn.metrics` module.  \n",
    "\n",
    "In the cell below, import the following functions:\n",
    "\n",
    "* `precision_score`\n",
    "* `recall_score`\n",
    "* `accuracy_score`\n",
    "* `f1_score`\n",
    "\n",
    "Then, use the `labels` and the predictions from each model (not the confusion matrices) to double check the performance of our functions above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model 1 Metrics:\n",
      "Precision: 0.94688363524716\n",
      "Recall: 0.8811428571428571\n",
      "Accuracy: 0.8822\n",
      "F1-Score: 0.9128311380790292\n",
      "----------------------------------------\n",
      "Model 2 Metrics:\n",
      "Precision: 0.744814462416746\n",
      "Recall: 0.5591428571428572\n",
      "Accuracy: 0.5573\n",
      "F1-Score: 0.6387596899224806\n",
      "----------------------------------------\n",
      "Model 3 Metrics:\n",
      "Precision: 0.8899127061105723\n",
      "Recall: 0.7864285714285715\n",
      "Accuracy: 0.7824\n",
      "F1-Score: 0.8349764902168968\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed here first!\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "m1 = (precision_score(labels, model1_preds),\n",
    "     recall_score(labels, model1_preds),\n",
    "     accuracy_score(labels, model1_preds),\n",
    "     f1_score(labels, model1_preds))\n",
    "\n",
    "m2 = (precision_score(labels, model2_preds),\n",
    "     recall_score(labels, model2_preds),\n",
    "     accuracy_score(labels, model2_preds),\n",
    "     f1_score(labels, model2_preds))\n",
    "\n",
    "m3 = (precision_score(labels, model3_preds),\n",
    "     recall_score(labels, model3_preds),\n",
    "     accuracy_score(labels, model3_preds),\n",
    "     f1_score(labels, model3_preds))\n",
    "\n",
    "preds = [m1, m2, m3]\n",
    "\n",
    "for ind, i in enumerate(preds):\n",
    "    print('-'*40)\n",
    "    print('Model {} Metrics:'.format(ind + 1))\n",
    "    print('Precision: {}'.format(preds[ind][0]))\n",
    "    print('Recall: {}'.format(preds[ind][1]))\n",
    "    print('Accuracy: {}'.format(preds[ind][2]))\n",
    "    print('F1-Score: {}'.format(preds[ind][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Reports\n",
    "\n",
    "Remember that table that you filled out above? It's called a **_Classification Report_**, and it turns out that sklearn can even create one of those for you! This classification report even breaks down performance by individual class predictions for your model. \n",
    "\n",
    "In closing, let's create some and interpret some classification reports using sklearn. Like everything else we've used this lab, you can find the `classification_report()` function inside the `sklearn.metrics` module.  This function takes in two required arguments: labels, and predictions. \n",
    "\n",
    "Complete the code in the cell below to create classification reports for each of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model 1 Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.88      0.82      3000\n",
      "          1       0.95      0.88      0.91      7000\n",
      "\n",
      "avg / total       0.89      0.88      0.88     10000\n",
      "\n",
      "----------------------------------------\n",
      "Model 2 Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.55      0.43      3000\n",
      "          1       0.74      0.56      0.64      7000\n",
      "\n",
      "avg / total       0.63      0.56      0.58     10000\n",
      "\n",
      "----------------------------------------\n",
      "Model 3 Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.77      0.68      3000\n",
      "          1       0.89      0.79      0.83      7000\n",
      "\n",
      "avg / total       0.81      0.78      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import classification_report below!\n",
    "from sklearn.metrics import classification_report\n",
    "preds = [model1_preds, model2_preds, model3_preds]\n",
    "\n",
    "for ind, i in enumerate(preds):\n",
    "    print('-'*40)\n",
    "    print(\"Model {} Classification Report:\".format(ind + 1))\n",
    "    print(classification_report(labels, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we manually calculated various evaluation metrics to help us evaluate classifier performance, and we also made use of preexisting tools inside of sklearn for the same purpose. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
